{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_plotly_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a9fd231fd151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure_factory\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\plotly\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0m_plotly_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimporters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrelative_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_plotly_utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chart studio helps to embed interactive plotly graphs in platforms outside jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "import chart_studio.tools as tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up the credentials from Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls.set_credentials_file(username='IshaGulati',api_key='Kk4iDM0uYjM8PuTV2I20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import unidecode\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate random colours\n",
    "#### Can be used by passing number of colours needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colours(number_of_colors):\n",
    "    '''\n",
    "    Simple function for random colours generation.\n",
    "    Input:\n",
    "        number_of_colors - integer value indicating the number of colours which are going to be generated.\n",
    "    Output:\n",
    "        Color in the following format: ['#E86DA4'] .\n",
    "    '''\n",
    "    colors = []\n",
    "    for i in range(number_of_colors):\n",
    "        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[0:15,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will remove the only null row present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Sentiment text for better visualizations\n",
    "### We have done dictionary mapping to reflect the textual meaning of the sentiment classes\n",
    "* 0: Negative\n",
    "* 1: Neutral\n",
    "* 2: Positive\n",
    "* 3: Can't Tell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {0:'Negative',1:'Neutral',2:'Positive',3:\"\"\"Can't tell\"\"\"}\n",
    "train_data['vis_sentiment'] = train_data['sentiment'].map(sentiment_dict)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vis = train_data.groupby('vis_sentiment').count()['tweet'].reset_index().sort_values(by='tweet',ascending=False)\n",
    "temp_vis.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis of the Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='vis_sentiment',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = go.Figure(go.Funnelarea(\n",
    "    text =temp_vis.vis_sentiment,\n",
    "    values = temp_vis.tweet,\n",
    "    title = {\"position\": \"top center\"}\n",
    "    ))\n",
    "funnel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that more than half of the tweets are classified as Neutral tweets\n",
    "- Less than 10% of tweets are in the negative and can't tell categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "- Character Counts for Positive and Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(funnel, filename='Funnel chart',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "for label, group in train_data.groupby('sentiment'):\n",
    "    sns.distplot(group['tweet'].str.len(), label=str(label), ax=ax)\n",
    "plt.xlabel('# of characters')\n",
    "plt.ylabel('density')\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_len']= train_data['tweet'].apply(len)\n",
    "data = [\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==0]['tweet_len'],\n",
    "        name='Negative'\n",
    "    ),\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==2]['tweet_len'],\n",
    "        name='Positive'\n",
    "    ),\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==1]['tweet_len'],\n",
    "        name = 'Neutral')\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title = 'Comparison of character count in Tweets '\n",
    ")\n",
    "char_box = go.Figure(data=data, layout=layout)\n",
    "char_box.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_count']= train_data['tweet'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet_len']= train_data['tweet'].apply(len)\n",
    "data = [\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==0]['word_count'],\n",
    "        name='Negative'\n",
    "    ),\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==2]['word_count'],\n",
    "        name='Positive'\n",
    "    ),\n",
    "    go.Box(\n",
    "        y=train_data[train_data['sentiment']==1]['word_count'],\n",
    "        name = 'Neutral')\n",
    "]\n",
    "layout = go.Layout(\n",
    "    title = 'Comparison of word count in Tweets '\n",
    ")\n",
    "word_count_box = go.Figure(data=data, layout=layout)\n",
    "word_count_box.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(word_count_box, filename='Word_count_box',auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Character Count : 280 characters since 2017, earlier it was 140 characters \n",
    "##### FOR THIS PARTICULAR DATASET WE CONSIDER 140 CHARACTERS AS THE DATA HAS DATES FROM 2011.\n",
    "What is Counted:\n",
    "\n",
    "   - Any character in the text of your post, including spaces\n",
    "   - Emojis (1 emoji registers as 2 characters)\n",
    "   - Hashtags\n",
    "   - Twitter handles (when mentioning an account)\n",
    "   - Links*\n",
    "\n",
    "What is not Counted:\n",
    "\n",
    "   - Visual content (images, GIFs, and videos)\n",
    "   - Polls\n",
    "   - Quote Tweets\n",
    "   - Twitter handles (only when you are replying to a Tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the corpus\n",
    "\n",
    "* *We see that there are common hastags such as #sxsw and #'?sxsw?' present in almost every tweet, we can remove them since they might not help us differentiate b/w sentiments*\n",
    "* The user handles for all the tweets have been replaced by **@mention**\n",
    "* The retweets are identified by character **RT**, hence these can be removed as well\n",
    "* The url's in tweets have been replaced by **{link}** and so these can be removed\n",
    "* Other cleaning steps involve removal of non aplhabets (digits, special symbols, punctuations\n",
    "\n",
    "\n",
    "#### Post Note : though the cleaning helped us in getting more insights, the model was only well trained when all the twitter data was used in it's most raw form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[0:20,'tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup - Decoding html to general text, will replace &amp and &quot to  & and \" \", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_encodings(x):\n",
    "    example1 = BeautifulSoup(x, 'lxml')\n",
    "    return example1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tweet\"] = train_data[\"tweet\"].apply(lambda x: remove_html_encodings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tweet'] = test_data['tweet'].apply(lambda x: remove_html_encodings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing all hashtags with SXSW/sxsw in it, as they are common to all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  train_data[train_data['sentiment'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data['tweet'] = train_data['tweet'].apply(lambda x:re.sub('[^\\s]*sxsw[^\\s]*','',x,flags=re.IGNORECASE))\n",
    "\n",
    "# train_data['hashtags'] = train_data['tweet'].str.findall(r'#.*?(?=\\s|$)') #finding and seperating all hashtags into a seperate column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tweet'] = test_data['tweet'].apply(lambda x:re.sub('[^\\s]*sxsw[^\\s]*','',x,flags=re.IGNORECASE))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Hashtags\n",
    "- Extracting all other hashtags for EDA, before we clean the data ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    x=x.str.lower()\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_hashtags = hashtag_extract(train_data['tweet'][train_data['sentiment'] == 2])\n",
    "\n",
    "\n",
    "negative_hashtags = hashtag_extract(train_data['tweet'][train_data['sentiment'] == 0])\n",
    "neutral_hashtags = hashtag_extract(train_data['tweet'][train_data['sentiment'] == 1])\n",
    "total_hashtags = hashtag_extract(train_data['tweet'])\n",
    "# unnesting list\n",
    "HT_positive = sum(positive_hashtags,[])\n",
    "HT_negative = sum(negative_hashtags,[])\n",
    "HT_neutral = sum(neutral_hashtags,[])\n",
    "HT_total= sum(total_hashtags,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_total)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 6) \n",
    "# plt.figure(figsize=(16,5))\n",
    "# ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "# ax.set(ylabel = 'Count')\n",
    "# plt.title('Count chart for All Hashtags')\n",
    "# plt.show()\n",
    "colors = ['#636efa','#ef553b','#00cc96','#ab63fa','#ffa15a','#19d3f3']\n",
    "# colors[1] = 'crimson'\n",
    "\n",
    "all_hashtags = go.Figure()\n",
    "all_hashtags.add_trace(go.Bar(x=d.Hashtag,\n",
    "    y=d.Count,\n",
    "    name='All Hashtags',\n",
    "    marker_color=colors\n",
    "))\n",
    "# all_hashtags = px.bar(d, x='Hashtag', y='Count',color ='Count')\n",
    "# all_hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(all_hashtags, filename='All Hashtags',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_positive)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 6) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.title('Count chart for Positive Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#636efa','#ef553b','#00cc96','#ab63fa','#ffa15a','#19d3f3']\n",
    "# colors[1] = 'crimson'\n",
    "\n",
    "positive_hashtags = go.Figure()\n",
    "positive_hashtags.add_trace(go.Bar(x=d.Hashtag,\n",
    "    y=d.Count,\n",
    "    name='Positive Hashtags',\n",
    "    marker_color=colors\n",
    "))\n",
    "positive_hashtags.update_layout(title_text='Positive Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(positive_hashtags, filename='Positive Hashtags',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_negative)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 6) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.title('Count chart for Negative Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#636efa','#ef553b','#00cc96','#ab63fa','#ffa15a','#19d3f3']\n",
    "# colors[1] = 'crimson'\n",
    "\n",
    "negative_hashtags = go.Figure()\n",
    "negative_hashtags.add_trace(go.Bar(x=d.Hashtag,\n",
    "    y=d.Count,\n",
    "    name='Negative Hashtags',\n",
    "    marker_color=colors\n",
    "))\n",
    "negative_hashtags.update_layout(title_text='Negative Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(negative_hashtags, filename='Negative Hashtags',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_neutral)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 6) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.title('Count chart for Neutral Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#636efa','#ef553b','#00cc96','#ab63fa','#ffa15a','#19d3f3']\n",
    "# colors[1] = 'crimson'\n",
    "\n",
    "neutral_hashtags = go.Figure()\n",
    "neutral_hashtags.add_trace(go.Bar(x=d.Hashtag,\n",
    "    y=d.Count,\n",
    "    name='Neutral Hashtags',\n",
    "    marker_color=colors\n",
    "))\n",
    "neutral_hashtags.update_layout(title_text='Neutral Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(neutral_hashtags, filename='Neutral Hashtags',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of contractions for contraction to expansion\n",
    "\n",
    "contraction_mapping = {\"aight\" : \"alright\",\n",
    " \"ain't\": \"am not\",\n",
    " \"amn't\" : \"am not\",\n",
    " \"aren't\": \"are not\",\n",
    " \"can't\": \"cannot\",\n",
    " \"'cause\" : \"because\",\n",
    " \"could've\": \"could have\",\n",
    " \"couldn't\" : \"could not\",\n",
    " \"couldn't've\" : \"could not have\", \n",
    " \"daren't\" : \"dare not\",\n",
    " \"daresn't\" : \"dare not\",\n",
    " \"dasn't\" : \"dare not\",\n",
    " \"didn't\" : \"did not\",\n",
    " \"doesn't\" : \"does not\",\n",
    " \"don't\" : \"do not\",\n",
    " \"d'ye\" : \"do you\",\n",
    " \"e'er\" : \"ever\",\n",
    " \"everybody's\" : \"everybody is\",\n",
    " \"everyone's\" : \"everyone is\",\n",
    " \"finna\":\"fixing to\",\n",
    " \"g'day\" : \"good day\",\n",
    " \"gimme\" : \"give me\",\n",
    " \"giv'n\": \"given\",\n",
    " \"gonna\":\"going to\",\n",
    " \"gon't\":\"go not\",\n",
    " \"gotta\":\"got to\",\n",
    " \"hadn't\":\"had not\",\n",
    " \"had've\":\"had have\",\n",
    " \"hasn't\":\"has not\",\n",
    " \"haven't\":\"have not\",\n",
    " \"he'd\":\"he would\",\n",
    " \"he'dn't've'd\":\"he would not have had\",\n",
    " \"he'll\":\"he will\",\n",
    " \"he's\":\"he is\",\n",
    " \"he've\":\"he have\",\n",
    " \"how'd\":\"how did\",\n",
    " \"howdy\":\"how do you do\",\n",
    " \"how'll\":\"how will\",\n",
    " \"how're\":\"how are\",\n",
    " \"how's\":\"how has\",\n",
    " \"i'd\": \"i would\",\n",
    " \"i'd've\":\"i would have\",\n",
    " \"i'll\": \"i will\",\n",
    " \"i'm\": \"i am\",\n",
    " \"i'm'a\": \"i am about to\",\n",
    " \"i'm'o\": \"i am going to\",\n",
    " \"innit\": \"is it not\",\n",
    " \"i've\": \"i have\",\n",
    " \"isn't\": \"is not\",\n",
    " \"it'd\": \"it would\",\n",
    " \"it'll\": \"it will\",\n",
    " \"it's\": \"it is\",\n",
    " \"let's\": \"let us\", \n",
    " \"ma'am\": \"madam\",\n",
    " \"mayn't\": \"may not\",\n",
    " \"may've\": \"may have\",\n",
    " \"methinks\" : \"me thinks\",\n",
    " \"mightn't\": \"might not\",\n",
    " \"might've\": \"might have\",\n",
    " \"mustn't\": \"must not\",\n",
    " \"mustn't've\": \"must not have\",\n",
    " \"must've\": \"must have\",\n",
    " \"needn't\": \"need not\",\n",
    " \"ne'er\":\"never\",\n",
    " \"o'clock\": \"of the clock\",\n",
    " \"o'er\": \"over\",\n",
    " \"ol'\": \"old\",\n",
    " \"oughtn't\":\"ought not\",\n",
    " \"'s\": \"is\",\n",
    " \"shalln't\":\"shall not\",\n",
    " \"shan't\":\"shall not\",\n",
    " \"she'd\":\"she would\",\n",
    " \"she'll\":\"she will\",\n",
    " \"she's\":\"she is\",\n",
    " \"should've\":\"should have\",\n",
    " \"shouldn't\":\"should not\",\n",
    " \"shouldn't've\":\"should not have\",\n",
    " \"somebody's\":\"somebody is\",\n",
    " \"someone's\":\"someone is\",\n",
    " \"something's\":\"something is\",\n",
    " \"so're\":\"so you are\",\n",
    " \"that'll\":\"that will\",\n",
    " \"that're\":\"that are\",\n",
    " \"that's\":\"that is\",\n",
    " \"that'd\":\"that had\",\n",
    " \"there'd\":\"there would\",\n",
    " \"there'll\":\"here shall\",\n",
    " \"there're\":\"there are\",\n",
    " \"there's\":\"there has\",\n",
    " \"these're\":\"these are\",\n",
    " \"these've\":\"these have\",\n",
    " \"they'd\":\"they would\",\n",
    " \"they'll\":\"they will\",\n",
    " \"they're\":\"they are\",\n",
    " \"they've\":\"they have\",\n",
    " \"this's\":\"this is\",\n",
    " \"those're\":\"those are\",\n",
    " \"those've\":\"those have\",\n",
    " \"'tis\":\"it is\",\n",
    " \"to've\":\"to have\",\n",
    " \"'twas\":\"it was\",\n",
    " \"wanna\":\"want to\",\n",
    " \"wasn't\":\"was not\",\n",
    " \"we'd\":\"we would\",\n",
    " \"we'd've\":\"we would have\",\n",
    " \"we'll\":\"we will\",\n",
    " \"we're\":\"we are\",\n",
    " \"we've\":\"we have\",\n",
    " \"weren't\":\"were not\",\n",
    " \"what'd\":\"what did\",\n",
    " \"what'll\":\"what will\",\n",
    " \"what're\":\"what are\",\n",
    " \"what's\":\"what is\",\n",
    " \"what've\":\"what have\",\n",
    " \"when's\":\"when is\",\n",
    " \"where'd\":\"where did\",\n",
    " \"where'll\":\"where will\",\n",
    " \"where're\":\"where are\",\n",
    " \"where's\":\"where has\",\n",
    " \"where've\":\"where have\",\n",
    " \"which'd\":\"which had\",\n",
    " \"which'll\":\"which shall\",\n",
    " \"which're\":\"which are\",\n",
    " \"which's\":\"which has\",\n",
    " \"which've\":\"which have\",\n",
    " \"who'd\":\"who would\",\n",
    " \"who'd've\":\"who would have\",\n",
    " \"who'll\":\"who will\",\n",
    " \"who're\":\"who are\",\n",
    " \"who's\":\"who has\",\n",
    " \"who've\":\"who have\",\n",
    " \"why'd\":\"why did\",\n",
    " \"why're\":\"why are\",\n",
    " \"why's\":\"why is\",\n",
    " \"won't\":\"will not\",\n",
    " \"would've\":\"would have\",\n",
    " \"wouldn't\":\"would not\",\n",
    " \"wouldn't've\":\"would not have\",\n",
    " \"y'all\":\"you all\",\n",
    " \"y'all'd've\":\"you all would have\",\n",
    " \"y'all'dn't've'd\":\"you all would not have had\",\n",
    " \"y'all're\":\"you all are\",\n",
    " \"you'd\":\"you would\",\n",
    " \"you'll\":\"you will\",\n",
    " \"you're\":\"you are\",\n",
    " \"you've\":\"you have\",\n",
    "  \" u \" : \" you\",\n",
    " \" ur \" : \" your\",\n",
    " \" n \": \" and \",\n",
    " \" w/ \" : \" with \",\n",
    " \" apples \": \" apple is \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "#     print(text)\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = re.sub(\"’\", \"'\", decoded)\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) #\"\"\" removes any words in square brackets\"\"\"\n",
    "    text = re.sub('{link}', '', text) #\"\"\"removes {link} from text \"\"\"\n",
    "    text = re.sub('@mention','',text) #removes user handles\n",
    "    text = re.sub('rt','',text) #removes RT as string\n",
    "    text = re.sub('<.*?>+', '', text) #\"\"\"removes any words in <___> \"\"\"\n",
    "#     text = re.sub('[^\\s]*sxsw[^\\s]*','',text) #removes all strings with sxsw\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #\"\"\"removes punctuations \"\"\"\n",
    "    \n",
    "    text = re.sub('\\n', '', text) #removes line breaks\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removes words with digits\n",
    "#     text = re.sub('[0-9a-zA-Z]*[^\\s0-9a-zA-Z]+[0-9a-zA-Z]*','',text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text) #spell_corrected\n",
    "    text = text.replace('\\r','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cleaned_text'] = train_data['tweet'].apply(lambda x:clean_text(x))\n",
    "train_data['cleaned_text'] = train_data['cleaned_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "train_data.loc[0:20,'cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['cleaned_text'] = test_data['tweet'].apply(lambda x:clean_text(x))\n",
    "test_data['cleaned_text'] = test_data['cleaned_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "test_data.loc[0:20,'cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying NER after first level of cleaning\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "pattern = [{\"label\":\"ORG\", \"pattern\": \"apple\"},\n",
    "           {\"label\":\"ORG\", \"pattern\": \"google\"},\n",
    "           {\"label\":\"ORG\", \"pattern\": \"facebook\"},\n",
    "           {\"label\":\"ORG\", \"pattern\": \"amazon\"},\n",
    "           {\"label\":\"ORG\", \"pattern\": \"microsoft\"}]\n",
    "\n",
    "ruler.add_patterns(pattern)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def ner(text):\n",
    "  doc = nlp(text)\n",
    "  return doc.ents\n",
    "\n",
    "train_data[\"named_entity_1\"] = train_data[\"tweet\"].apply(lambda x : ([(word.text, word.label_) for word in ner(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"organizations\"] = train_data[\"tweet\"].apply(lambda x : ([(word.text) for word in ner(x) if word.label_ == \"ORG\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "dict_org = {}\n",
    "def create_set(x):\n",
    "    for i in x:\n",
    "        if i in dict_org:\n",
    "            dict_org[i] += 1\n",
    "        else:\n",
    "            dict_org[i] = 1\n",
    "    \n",
    "train_data[\"organizations\"].apply(lambda x:create_set(x))\n",
    "sorted(dict_org.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = {'org_apple':['apple','iphone','ipads','ipad','iphones','itunes','ipad2','ios','mac','macos','macbook','ipod'],\n",
    "        'org_google':['google','android','andoid','nexus'],\n",
    "        'org_uber':['uber','uberguide'],\n",
    "        'org_microsoft':['microsoft','bing','windows'],\n",
    "         'org_facebook':['facebook']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in orgs:\n",
    "    train_data[key] = 0\n",
    "train_data.head()\n",
    "# train_data.drop(['Apple','Google','Uber','Microsoft','Facebook'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[0,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in train_data.iterrows():\n",
    "    for word in row['cleaned_text'].split():        \n",
    "        for key in orgs:             \n",
    "            if word in orgs[key]:\n",
    "                train_data.loc[index,key] = 1\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"location\"] = train_data[\"tweet\"].apply(lambda x : ([(word.text) for word in ner(x) if word.label_ == \"GPE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"person\"] = train_data[\"tweet\"].apply(lambda x : ([(word.text) for word in ner(x) if word.label_ == \"PERSON\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_location = {}\n",
    "def create_location_set(x):\n",
    "    for i in x:\n",
    "        if i in dict_location:\n",
    "            dict_location[i] += 1\n",
    "        else:\n",
    "            dict_location[i] = 1\n",
    "    \n",
    "train_data[\"location\"].apply(lambda x:create_location_set(x))\n",
    "sorted(dict_location.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_person = {}\n",
    "def create_person_set(x):\n",
    "    for i in x:\n",
    "        if i in dict_person:\n",
    "            dict_person[i] += 1\n",
    "        else:\n",
    "            dict_person[i] = 1\n",
    "    \n",
    "train_data[\"person\"].apply(lambda x:create_person_set(x))\n",
    "sorted(dict_person.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=[]\n",
    "positive=[]\n",
    "negative=[]\n",
    "neutral=[]\n",
    "for key in orgs:\n",
    "    count_total = train_data[train_data[key]==1][key].sum()\n",
    "    count_positive = train_data[(train_data[key]==1) & (train_data['sentiment']==2)][key].sum()\n",
    "    count_negative = train_data[(train_data[key]==1) & (train_data['sentiment']==0)][key].sum()\n",
    "    count_neutral = train_data[(train_data[key]==1) & (train_data['sentiment']==1)][key].sum()\n",
    "    total.append(count_total)\n",
    "    positive.append(count_positive)\n",
    "    negative.append(count_negative)\n",
    "    neutral.append(count_neutral)\n",
    "x = ['Apple','Google','Uber','Microsoft','Facebook']\n",
    "plot_org_data = pd.DataFrame(list(zip(x, total,positive,negative,neutral)),columns=['Organisation','Tweet_Count','Positive_Count','Negative_Count','Neutral_Count'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data = plot_org_data.sort_values(by='Tweet_Count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_by_org = go.Figure(data=[\n",
    "    go.Bar(x=plot_org_data['Organisation'], y=plot_org_data['Tweet_Count'],),\n",
    "])\n",
    "# Change the bar mode\n",
    "tweet_by_org.update_layout(barmode='group')\n",
    "tweet_by_org.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tweet_by_org, filename='Organisation_tweet_count',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data_t = plot_org_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data_t.columns = plot_org_data_t.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data_t = plot_org_data_t.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data_t.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_org_data_t = plot_org_data_t.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_by_sent_org = go.Figure(data=[\n",
    "    go.Bar(name='Neutral', x=plot_org_data['Organisation'], y=plot_org_data['Neutral_Count']),\n",
    "    go.Bar(name='Positive', x=plot_org_data['Organisation'], y=plot_org_data['Positive_Count']),\n",
    "    go.Bar(name='Negative', x=plot_org_data['Organisation'], y=plot_org_data['Negative_Count'])\n",
    "])\n",
    "# Change the bar mode\n",
    "tweet_by_sent_org.update_layout(barmode='group')\n",
    "tweet_by_sent_org.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tweet_by_sent_org, filename='Organisation wise Sentiment Count',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_pie = px.pie(plot_org_data_t, values='Apple', names=plot_org_data_t.index, title='Apple')\n",
    "apple_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py.plot(apple_pie, filename='Sentiment Distribution for Apple',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_pie = px.pie(plot_org_data_t, values='Google', names=plot_org_data_t.index, title='Google')\n",
    "google_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(google_pie, filename='Sentiment Distribution for Google',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoft_pie = px.pie(plot_org_data_t, values='Microsoft', names=plot_org_data_t.index, title='Microsoft')\n",
    "microsoft_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(microsoft_pie, filename='Sentiment Distribution for Microsoft',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_pie = px.pie(plot_org_data_t, values='Uber', names=plot_org_data_t.index, title='Uber')\n",
    "uber_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(uber_pie, filename='Sentiment Distribution for Uber',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_pie = px.pie(plot_org_data_t, values='Facebook', names=plot_org_data_t.index, title='Facebook')\n",
    "facebook_pie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(facebook_pie, filename='Sentiment Distribution for Facebook',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['temp_list'] = train_data['cleaned_text'].apply(lambda x:str(x).split())\n",
    "top = Counter([item for sublist in train_data['temp_list'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(x):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'amp', \"quot\",\"via\",\"will\"}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "    return [y for y in x if y not in stopwords]\n",
    "train_data['temp_list'] = train_data['temp_list'].apply(lambda x:remove_stopword(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in train_data['temp_list'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(100))\n",
    "temp = temp.iloc[1:,:]\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_common = px.treemap(temp.head(20), path=['Common_words'], values='count',title='Tree of Most Common Words')\n",
    "tree_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tree_common, filename='Most Common Words',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_sent = train_data[train_data['sentiment']==0]\n",
    "Positive_sent = train_data[train_data['sentiment']==2]\n",
    "Neutral_sent = train_data[train_data['sentiment']==1]\n",
    "# Canttell_sent = train_data[train_data['sentiment']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(100))\n",
    "temp_positive.columns = ['Common_words','count']\n",
    "temp_positive.style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_pos_common = px.bar(temp_positive.head(20), x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n",
    "             width=700, height=700,color='Common_words')\n",
    "bar_pos_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_pos_common, filename='Positive Common Words',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pos_common = px.treemap(temp_positive.head(20), path=['Common_words'], values='count',title='Tree Of Most Common Positive Words')\n",
    "tree_pos_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tree_pos_common, filename='Positive Common Words Tree',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\n",
    "temp_negative = pd.DataFrame(top.most_common(100))\n",
    "temp_negative = temp_negative.iloc[1:,:]\n",
    "temp_negative.columns = ['Common_words','count']\n",
    "temp_negative.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_neg_common = px.bar(temp_negative.head(20), x=\"count\", y=\"Common_words\", title='Most Commmon Negative Words', orientation='h', \n",
    "             width=700, height=700,color='Common_words')\n",
    "bar_neg_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_neg_common, filename='Negative Common Words',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_neg_common = px.treemap(temp_negative.head(20), path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\n",
    "tree_neg_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tree_neg_common, filename='Negative Common Words Tree',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MosT common Neutral words\n",
    "top = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\n",
    "temp_neutral = pd.DataFrame(top.most_common(100))\n",
    "temp_neutral = temp_neutral.loc[1:,:]\n",
    "temp_neutral.columns = ['Common_words','count']\n",
    "temp_neutral.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_neu_common = px.bar(temp_neutral.head(20), x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n",
    "             width=700, height=700,color='Common_words')\n",
    "bar_neu_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_neu_common, filename='Neutral Common Words',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_neu_common = px.treemap(temp_neutral.head(20), path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\n",
    "tree_neu_common.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(tree_neu_common, filename='Neutral Common Words Tree',auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now generate bigrams and trigrams to see what phrases and words were used in Positive and Negative Tweete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams (text,n=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams= zip(*[token[i:] for i in range(n)])\n",
    "    return[' '.join(ngram) for ngram in ngrams]\n",
    "N=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "positive_bigrams = defaultdict(int)\n",
    "negative_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in Positive_sent['cleaned_text']:\n",
    "    for word in generate_ngrams(tweet, n=2):\n",
    "        positive_bigrams[word] += 1\n",
    "        \n",
    "for tweet in Negative_sent['cleaned_text']:\n",
    "    for word in generate_ngrams(tweet, n=2):\n",
    "        negative_bigrams[word] += 1\n",
    "        \n",
    "positive_bigrams_df = pd.DataFrame(sorted(positive_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "negative_bigrams_df = pd.DataFrame(sorted(negative_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "positive_bigrams_df = positive_bigrams_df.sort_values(by = 1,ascending=True)\n",
    "negative_bigrams_df = negative_bigrams_df.sort_values(by = 1,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_trigrams = defaultdict(int)\n",
    "negative_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in Positive_sent['cleaned_text']:\n",
    "    for word in generate_ngrams(tweet, n=3):\n",
    "        positive_trigrams[word] += 1\n",
    "        \n",
    "for tweet in Negative_sent['cleaned_text']:\n",
    "    for word in generate_ngrams(tweet, n=3):\n",
    "        negative_trigrams[word] += 1\n",
    "        \n",
    "positive_trigrams_df = pd.DataFrame(sorted(positive_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "negative_trigrams_df = pd.DataFrame(sorted(negative_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "positive_trigrams_df = positive_trigrams_df.sort_values(by = 1,ascending=True)\n",
    "negative_trigrams_df = negative_trigrams_df.sort_values(by = 1,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_positive_bigrams = px.bar(positive_bigrams_df.head(20), x=positive_bigrams_df[1].values[12016:12031], y=positive_bigrams_df[0].values[12016:12031], title='Positive Bigrams', orientation='h', color=positive_bigrams_df[0].values[12016:12031])\n",
    "bar_positive_bigrams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_positive_bigrams, filename='Positive Bigrams',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_negative_bigrams = px.bar(negative_bigrams_df.head(20), x=negative_bigrams_df[1].values[2939:2954], y=negative_bigrams_df[0].values[2939:2954], title='Negative Bigrams', orientation='h', color=negative_bigrams_df[0].values[2939:2954])\n",
    "bar_negative_bigrams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_negative_bigrams, filename='Negative Bigrams',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_trigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_positive_trigrams = px.bar(positive_bigrams_df.head(20), x=positive_trigrams_df[1].values[12343:12358], y=positive_trigrams_df[0].values[12343:12358], title='Positive Trigrams', orientation='h', color=positive_trigrams_df[0].values[12343:12358])\n",
    "bar_positive_trigrams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_positive_trigrams, filename='Positive Trigrams',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_trigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_negative_trigrams = px.bar(negative_trigrams_df.head(20), x=negative_trigrams_df[1].values[2771:2786], y=negative_trigrams_df[0].values[2771:2786], title='Negative Trigrams', orientation='h', color=negative_trigrams_df[0].values[2771:2786])\n",
    "bar_negative_trigrams.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.plot(bar_negative_trigrams, filename='Negative Trigrams',auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bigrams\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=positive_bigrams_df[0].values[:30], x=positive_bigrams_df[1].values[:30], ax=axes[0], color='turquoise')\n",
    "sns.barplot(y=negative_bigrams_df[0].values[:30], x=negative_bigrams_df[1].values[:30], ax=axes[1], color='orange')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=20)\n",
    "    axes[i].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[0].set_title(f'Top {30} most common bigrams in Positive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {30} most common bigrams in Negative Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting trigrams\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=positive_trigrams_df[0].values[:30], x=positive_trigrams_df[1].values[:30], ax=axes[0], color='turquoise')\n",
    "sns.barplot(y=negative_trigrams_df[0].values[:30], x=negative_trigrams_df[1].values[:30], ax=axes[1], color='orange')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=20)\n",
    "    axes[i].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[0].set_title(f'Top {30} most common trigrams in Positive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {30} most common trigrams in Negative Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = [word for word_list in train_data['temp_list'] for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_unique(sentiment,numwords,raw_words):\n",
    "    '''\n",
    "    Input:\n",
    "        sentiment - Sentiment category (ex. 'Neutral');\n",
    "        numwords - how many specific words do you want to see in the final result; \n",
    "        raw_words - list  for item in train_data[train_data.sentiment == sentiment]['temp_list']:\n",
    "    Output: \n",
    "        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n",
    "\n",
    "    '''\n",
    "    allother = []\n",
    "    for item in train_data[train_data.sentiment != sentiment]['temp_list']:\n",
    "        for word in item:\n",
    "            allother .append(word)\n",
    "    allother  = list(set(allother ))\n",
    "    \n",
    "    specificnonly = [x for x in raw_text if x not in allother]\n",
    "    \n",
    "    mycounter = Counter()\n",
    "    \n",
    "    for item in train_data[train_data.sentiment == sentiment]['temp_list']:\n",
    "        for word in item:\n",
    "            mycounter[word] += 1\n",
    "    keep = list(specificnonly)\n",
    "    \n",
    "    for word in list(mycounter):\n",
    "        if word not in keep:\n",
    "            del mycounter[word]\n",
    "    \n",
    "    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n",
    "    \n",
    "    return Unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Positive= words_unique(2, 100, raw_text)\n",
    "print(\"The top 20 unique words in Positive Tweets are:\")\n",
    "Unique_Positive.head(20).style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palettable.colorbrewer.qualitative import Pastel1_7\n",
    "top_20_positive = Unique_Positive.head(20)\n",
    "plt.figure(figsize=(16,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(top_20_positive['count'], labels=top_20_positive.words, colors=Pastel1_7.hex_colors)\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.title('Plot Of Unique Positive Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Negative = words_unique(0, 100, raw_text)\n",
    "print(\"The top 20 unique words in Negative Tweets are:\")\n",
    "Unique_Negative.head(20).style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palettable.colorbrewer.qualitative import Pastel1_7\n",
    "top_20_negative = Unique_Negative.head(20)\n",
    "plt.figure(figsize=(16,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(top_20_negative['count'], labels=top_20_negative.words, colors=Pastel1_7.hex_colors)\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.title('Plot Of Unique Negative Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Neutral= words_unique(1, 50, raw_text)\n",
    "print(\"The top 10 unique words in Neutral Tweets are:\")\n",
    "Unique_Neutral.head(10).style.background_gradient(cmap='Oranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from palettable.colorbrewer.qualitative import Pastel1_7\n",
    "top_10_neutral = Unique_Neutral.head(10)\n",
    "plt.figure(figsize=(16,10))\n",
    "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(top_10_neutral['count'], labels=top_10_neutral.words, colors=Pastel1_7.hex_colors)\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.title('DoNut Plot Of Unique Neutral Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(text, mask, max_words=200, max_font_size=100, figure_size=(15,10), color = 'white',\n",
    "                   title = None, title_size=40, image_color=False):\n",
    "#     print(text)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {\"bitlyhmiiga\",'scheen','spos', 'needing','filteraa', 'lanzara', 'ningun', 'producto','cst', 'youaare', 'zlf', 'sat','aaps','offersaa' }\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color=color,\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=400, \n",
    "                    height=200,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "positive_hash_corpus = ''\n",
    "for sublist in positive_hashtags:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "positive_hash_corpus = ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_hash_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(positive_hash_corpus,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Positive Hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "negative_hash_corpus = ''\n",
    "for sublist in negative_hashtags:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "negative_hash_corpus = ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(negative_hash_corpus,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Negative Hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "neutral_hash_corpus = ''\n",
    "for sublist in neutral_hashtags:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "neutral_hash_corpus = ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(neutral_hash_corpus,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = ''\n",
    "for i in temp.Common_words:\n",
    "#     print(i)\n",
    "    common_words += \"\".join(i)+\" \"\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(common_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Most Used Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pos_words = ''\n",
    "for i in temp_positive.Common_words:\n",
    "#     print(i)\n",
    "    common_pos_words += \"\".join(i)+\" \"\n",
    "common_pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(common_pos_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Most Used Positive Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a corpus of unique\n",
    "positive_words = ''\n",
    "for i in Unique_Positive.words:\n",
    "#     print(i)\n",
    "    positive_words += \"\".join(i)+\" \"\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(positive_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Unique Positive Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_neg_words = ''\n",
    "for i in temp_negative.Common_words:\n",
    "#     print(i)\n",
    "    common_neg_words += \"\".join(i)+\" \"\n",
    "common_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(common_neg_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Most Used Negative Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a corpus of unique\n",
    "negative_words = ''\n",
    "for i in Unique_Negative.words:\n",
    "#     print(i)\n",
    "    negative_words += \"\".join(i)+\" \"\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(negative_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Unique Negative Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_neu_words = ''\n",
    "for i in temp_neutral.Common_words:\n",
    "#     print(i)\n",
    "    common_neu_words += \"\".join(i)+\" \"\n",
    "common_neu_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(common_neu_words,mask=pos_mask,color='white',max_font_size=200,title_size=30,title=\"WordCloud of Most Used Neutral Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a corpus of common_words\n",
    "neutral_words = ''\n",
    "for i in Unique_Neutral.words:\n",
    "#     print(i)\n",
    "    neutral_words += \"\".join(i)+\" \"\n",
    "neutral_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= '../data/masks-for-wordclouds/'\n",
    "pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\n",
    "plot_wordcloud(neutral_words,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Unique Neutral Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['tweet_id'] != 5025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "train_data['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in train_data['tweet']] \n",
    "print(train_data['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tokenized_text'] = [simple_preprocess(line,deacc=True) for line in test_data['tweet']]\n",
    "print(test_data['tokenized_text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "# Get the stemmed_tokens\n",
    "train_data['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in train_data['tokenized_text'] ]\n",
    "train_data['stemmed_tokens'].head(10)\n",
    "\n",
    "test_data['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in test_data['tokenized_text']]\n",
    "test_data['stemmed_tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# stop_words = list(set(stopwords.words('english')))\n",
    "# train_data['tokenized_data'] = train_data['cleaned_text'].apply(lambda row: word_tokenize(row))\n",
    "# test_data['tokenized_data'] = test_data['cleaned_text'].apply(lambda row: word_tokenize(row))\n",
    "\n",
    "\n",
    "# # stopword removal\n",
    "# train_data['tokenized_data'] = train_data['tokenized_data'].apply(lambda row: [word for word in row if word not in stop_words])\n",
    "# # train_data['tokenized_data'] = train_data['tokenized_data'].apply(lambda row: [])\n",
    "# test_data['tokenized_data'] = test_data['tokenized_data'].apply(lambda row: [word for word in row if word not in stop_words])\n",
    "\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# stemming words\n",
    "# stemmer = PorterStemmer()\n",
    "corpus = []\n",
    "train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x:[porter.stem(i) for i in x])\n",
    "train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x:' '.join(x))\n",
    "for i in train_data.tokenized_text:\n",
    "    corpus.append(i)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x:[porter.stem(i) for i in x])\n",
    "test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x:' '.join(x))\n",
    "\n",
    "# train_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(max_features=1500)\n",
    "\n",
    "\n",
    "# # Independent variable\n",
    "# X = (cv.fit_transform(corpus)).toarray()\n",
    "\n",
    "\n",
    "# # dependent variable\n",
    "# y = train_data['sentiment']\n",
    "\n",
    "# # Counts\n",
    "# count = y.value_counts()\n",
    "# print(count)\n",
    "\n",
    "# # Split the dataset\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/traindata_check.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_check = pd.read_csv('../data/traindata_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_check.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Creating Polarity Column using TextBlob\n",
    "tb_polarity = []\n",
    "for sentence in train_data['tokenized_data']:\n",
    "    temp = TextBlob(sentence)\n",
    "    tb_polarity.append(temp.sentiment[0])\n",
    "train_data['polarity'] = tb_polarity\n",
    "\n",
    "test_polarity =[]\n",
    "for sentence in test_data['tokenized_data']:\n",
    "    temp = TextBlob(sentence)\n",
    "    test_polarity.append(temp.sentiment[0])\n",
    "# print(tb_polarity)\n",
    "test_data['polarity'] = test_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['polarity'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tweets_train = tfidf_vectorizer.fit_transform(train_data['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test = tfidf_vectorizer.transform(test_data['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tweets_train.toarray()\n",
    "# X = pd.DataFrame(X)\n",
    "\n",
    "# X.index = train_data.index\n",
    "\n",
    "# X['polarity'] = train_data['polarity']\n",
    "# train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "# ratio to split into training and test set\n",
    "ratio = int(len(train_data)*0.75)\n",
    "\n",
    "# logistic regression model\n",
    "logreg = LogisticRegression(random_state=2) \n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "# tfidf_vedtorizer = TfidfVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')\n",
    "\n",
    "# # fit and transform tweets\n",
    "# tweets = tfidf_vedtorizer.fit_transform(train_data['tokenized_data'])\n",
    "\n",
    "\n",
    "# split into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,train_data.sentiment, test_size=0.25,random_state=22)\n",
    "# X_train = tweets[:ratio,:]\n",
    "# X_test = tweets[ratio:,:]\n",
    "# y_train = train_data['sentiment'].iloc[:ratio]\n",
    "# y_test = train_data['sentiment'].iloc[ratio:]\n",
    "\n",
    "# fit on training data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "prediction = logreg.predict(X_test)\n",
    "# prediction_int = (prediction[:,1] >= 0.3).astype(int)\n",
    "\n",
    "# print out accuracy\n",
    "f1 = f1_score(y_test,prediction,average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# Instantiate calssifier\n",
    "rf = RandomForestClassifier(random_state=2)\n",
    "\n",
    "# fit model on training data\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy score\n",
    "score = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "# calculate the precision\n",
    "# precision = precision_score(y_test,y_pred)\n",
    "\n",
    "# display 'score' and 'precision'\n",
    "\n",
    "print(score)\n",
    "# print(precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_grid = gridF.predict(X_test)\n",
    "score_grid_rf = f1_score(y_test,y_pred_rf_grid,average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_grid_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate smote\n",
    "smote = SMOTE(random_state=9)\n",
    "\n",
    "# fit_sample onm training data\n",
    "X_smote,y_smote = smote.fit_sample(X_train,y_train)\n",
    "\n",
    "# fit modelk on training data\n",
    "rf.fit(X_smote,y_smote)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy score\n",
    "score = f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "# calculate the precision\n",
    "# precision = precision_score(y_test,y_pred)\n",
    "\n",
    "# display precision and score\n",
    "print(score)\n",
    "# print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(rf, hyperF, cv = 3, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "gridF.fit(X_smote, y_smote)\n",
    "print(gridF.best_params_)\n",
    "print(gridF.best_estimator_)\n",
    "\n",
    "rf_grid_predict = gridF.predict(X_test)\n",
    "rf_grid_score = f1_score(y_test,rf_grid_predict)\n",
    "print(rf_grid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "# svm = LinearSVC(random_state = 42)\n",
    "# svm.fit(X_train, y_train)\n",
    "# y_pred = svm.predict(X_test)\n",
    "# score = f1_score(y_test,y_pred,average='weighted')\n",
    "# print(score)\n",
    "\n",
    "param_grid = {'C': [0.1, 0.1,1], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "grid = GridSearchCV(\n",
    "    SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_smote, y_smote) \n",
    "\n",
    "print(grid.best_params_) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "print(grid.best_estimator_) \n",
    "\n",
    "\n",
    "grid_predictions = grid.predict(X_test) \n",
    "  \n",
    "# print classification report \n",
    "print(f1_score(y_test, grid_predictions,average='weighted')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=1, gamma = 1, kernel = 'rbf')\n",
    "svc_model.fit(X_train,y_train)\n",
    "score_svc  = f1_score(y_test,svc_model.predict(X_test),average='weighted')\n",
    "print(score_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model_smote = SVC(C=1, gamma = 1, kernel = 'rbf')\n",
    "svc_model_smote.fit(X_smote,y_smote)\n",
    "score_svc  = f1_score(y_test,svc_model_smote.predict(X_test),average='weighted')\n",
    "print(score_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y= pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test_data['tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.index.equals(test_data.index)\n",
    "# y.index.intersection(test_data.index).empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y['polarity'] = test_data['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=9)\n",
    "X_smote,y_smote = smote.fit_sample(X,y_train)\n",
    "svc_model = SVC(C=1, gamma = 1, kernel = 'rbf')\n",
    "svc_model.fit(X_smote,y_smote)\n",
    "y_pred_svc = svc_model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=9)\n",
    "\n",
    "# fit_sample onm training data\n",
    "X_smote,y_smote = smote.fit_sample(X,y_train)\n",
    "\n",
    "# Instantiate calssifier\n",
    "rf_test = RandomForestClassifier(random_state=2)\n",
    "\n",
    "# fit model on training data\n",
    "rf.fit(X_smote,y_smote)\n",
    "\n",
    "#\n",
    "predict on test data\n",
    "y_pred_rf = rf.predict(y)\n",
    "\n",
    "# calculate the accuracy score\n",
    "# score = f1_score(y_test,y_pred,average='micro')\n",
    "\n",
    "# calculate the precision\n",
    "# precision = precision_score(y_test,y_pred)\n",
    "\n",
    "# display 'score' and 'precision'\n",
    "\n",
    "# print(score)\n",
    "# print(precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(y_pred_svc,columns=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_trial_10 = pd.concat([ID,prediction['sentiment']],1)\n",
    "submission_trial_10.to_csv('../data/submission_trial_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = pd.read_csv('../data/submission_trial_10.csv')\n",
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
